

<!DOCTYPE html>
<html class="writer-html5" lang="en" data-content_root="./">
<head>
  <meta charset="utf-8" /><meta name="viewport" content="width=device-width, initial-scale=1" />

  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>Single fidelity surrogates &mdash; mfpml 1.0.0 documentation</title>
      <link rel="stylesheet" type="text/css" href="_static/pygments.css?v=fa44fd50" />
      <link rel="stylesheet" type="text/css" href="_static/css/theme.css?v=e59714d7" />
      <link rel="stylesheet" type="text/css" href="_static/readthedocs-custom.css" />

  
      <script src="_static/documentation_options.js?v=8d563738"></script>
      <script src="_static/doctools.js?v=9bcbadda"></script>
      <script src="_static/sphinx_highlight.js?v=dc90522c"></script>
      <script async="async" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <script src="_static/js/theme.js"></script>
    <link rel="index" title="Index" href="genindex.html" />
    <link rel="search" title="Search" href="search.html" />
    <link rel="next" title="Multi fidelity surrogates" href="multi_fidelity_kriging_models.html" />
    <link rel="prev" title="Problems class structure" href="problems.html" /> 
</head>

<body class="wy-body-for-nav"> 
  <div class="wy-grid-for-nav">
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >

          
          
          <a href="index.html" class="icon icon-home">
            mfpml
          </a>
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" aria-label="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>
        </div><div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="Navigation menu">
              <p class="caption" role="heading"><span class="caption-text">Overview:</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="readme.html">Summary</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Get started:</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="get_started.html">Installation</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Design of experiments:</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="design_of_experiments.html">Single-Fidelity Samplers</a></li>
<li class="toctree-l1"><a class="reference internal" href="design_of_experiments.html#multi-fidelity-samplers">Multi-Fidelity Samplers</a></li>
<li class="toctree-l1"><a class="reference internal" href="design_of_experiments.html#summary">Summary</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Problems:</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="problems.html">Problems class structure</a></li>
<li class="toctree-l1"><a class="reference internal" href="problems.html#single-fidelity-problems">Single fidelity problems</a></li>
<li class="toctree-l1"><a class="reference internal" href="problems.html#multi-fidelity-problems">Multi fidelity problems</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Surrogates:</span></p>
<ul class="current">
<li class="toctree-l1 current"><a class="current reference internal" href="#">Single fidelity surrogates</a><ul>
<li class="toctree-l2"><a class="reference internal" href="#kriging-model">Kriging model</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#basics-of-kriging-model">1. basics of Kriging model</a></li>
<li class="toctree-l3"><a class="reference internal" href="#negative-log-likelihood">2. Negative log-likelihood</a></li>
<li class="toctree-l3"><a class="reference internal" href="#inference">3. Inference</a></li>
<li class="toctree-l3"><a class="reference internal" href="#implementation">4. Implementation</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="#gaussian-process-regression-for-noisy-data">Gaussian process regression for noisy data</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#basics-of-general-gaussian-process-regression">1. basics of general Gaussian process regression</a></li>
<li class="toctree-l3"><a class="reference internal" href="#id1">1. Implementation</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="multi_fidelity_kriging_models.html">Multi fidelity surrogates</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Optimization:</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="optimization.html">Evolutionary algorithms</a></li>
<li class="toctree-l1"><a class="reference internal" href="optimization.html#single-fidelity-bayesian-optimization">Single-fidelity Bayesian optimization</a></li>
<li class="toctree-l1"><a class="reference internal" href="optimization.html#multi-fidelity-bayesian-optimization">Multi-fidelity Bayesian optimization</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">API Reference:</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="mfpml.html">API Documentation</a></li>
</ul>

        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap"><nav class="wy-nav-top" aria-label="Mobile navigation menu" >
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="index.html">mfpml</a>
      </nav>

      <div class="wy-nav-content">
        <div class="rst-content">
          <div role="navigation" aria-label="Page navigation">
  <ul class="wy-breadcrumbs">
      <li><a href="index.html" class="icon icon-home" aria-label="Home"></a></li>
      <li class="breadcrumb-item active">Single fidelity surrogates</li>
      <li class="wy-breadcrumbs-aside">
      </li>
  </ul>
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
             
  <section id="single-fidelity-surrogates">
<h1>Single fidelity surrogates<a class="headerlink" href="#single-fidelity-surrogates" title="Link to this heading">¶</a></h1>
<p>Surrogates is one of the most important part in the multi-fidelity optimization.
In this module, we implement the surrogates for single fidelity and multi fidelity.
For single fidelity, we implement the following surrogates:</p>
<section id="kriging-model">
<h2>Kriging model<a class="headerlink" href="#kriging-model" title="Link to this heading">¶</a></h2>
<p>Kriging model is a Gaussian process regression model with a constant mean function and a squared exponential kernel.
The Kriging model is implemented in <code class="xref py py-attr docutils literal notranslate"><span class="pre">GaussianProcessRegression</span></code> class.</p>
<section id="basics-of-kriging-model">
<h3>1. basics of Kriging model<a class="headerlink" href="#basics-of-kriging-model" title="Link to this heading">¶</a></h3>
<p>A Gaussian process if completely specified by its mean function and alert{correlation} function, which can be defined as:</p>
<div class="math notranslate nohighlight">
\[m(\mathbf{x}) = \mathbb{E}\left [ f \left( \mathbf{x} \right) \right]\]</div>
<div class="math notranslate nohighlight">
\[k(\mathbf{x}, \mathbf{x}') = \mathbb{E}\left [ (f (\mathbf{x}) - m(\mathbf{x}) )(f(\mathbf{x}') - m(\mathbf{x}'))\right]\]</div>
<p>Therefore, Gaussian process can be write as:</p>
<div class="math notranslate nohighlight">
\[f(\mathbf{x}) \sim  \mathcal{GP} (m(\mathrm{x}), k(\mathrm{x}, \mathrm{x}'))\]</div>
<p>Usually, the RBF kernel function is defined as:</p>
<div class="math notranslate nohighlight">
\[k(\mathrm{x}^{i}, \mathrm{x}^{j}) = exp\left(-\sum_{d=1}^{k} \theta_d \left({x}_{d}^{i}-{x}_{d}^{j}\right)^{p_d}\right)\]</div>
<p>where <span class="math notranslate nohighlight">\(k\)</span> is the dimension of the problem, <span class="math notranslate nohighlight">\(\mathrm{\theta}\)</span>  and <span class="math notranslate nohighlight">\(\mathrm{p}\)</span> are two <span class="math notranslate nohighlight">\(k\)</span>-dimensional vectors of hyper-parameter
The correlation matrix is defined as:</p>
<div class="math notranslate nohighlight">
\[\begin{split}\mathrm{K}(\mathrm{X,X}) = \begin{bmatrix}
k(\mathrm{x}^{1}, \mathrm{x}^{1}) &amp; ... &amp; k(\mathrm{x}^{1}, \mathrm{x}^{n})\\
... &amp; ... &amp; ...\\
k(\mathrm{x}^{n}, \mathrm{x}^{1})&amp; ...&amp; k(\mathrm{x}^{n}, \mathrm{x}^{n})
\end{bmatrix}\end{split}\]</div>
<p>where <span class="math notranslate nohighlight">\(n\)</span> is the number of samples</p>
</section>
<section id="negative-log-likelihood">
<h3>2. Negative log-likelihood<a class="headerlink" href="#negative-log-likelihood" title="Link to this heading">¶</a></h3>
<p>The negative log-likelihood of the Gaussian process is defined as:</p>
<div class="math notranslate nohighlight">
\[\ln(L) =-\frac{n}{2}\ln(2\pi) - \frac{n}{2}\ln(\sigma_e^2) - \frac{1}{2}\ln|\mathbf{K}|  -\frac{(\mathbf{y}-\mathbf{1}\mu)^T\mathbf{K}^{-1}(\mathbf{y}-\mathbf{1}\mu)}{2\sigma_e^2}\]</div>
<p>in which, <span class="math notranslate nohighlight">\(\mathbf{y}\)</span> is the vector of observations, <span class="math notranslate nohighlight">\(\mathbf{1}\)</span> is a vector of ones, <span class="math notranslate nohighlight">\(\mu\)</span> is the mean function, <span class="math notranslate nohighlight">\(\sigma_e^2\)</span> is the variance of the noise, <span class="math notranslate nohighlight">\(\mathbf{K}\)</span> is the correlation matrix.
Overall, it has 2n + 2 parameters to be optimized, furthermore, the hyper-parameters can be estimated with the following two steps:</p>
<ol class="arabic simple">
<li><p>step 1: take derivative of the negative log-likelihood with respect to the hyper-parameters <span class="math notranslate nohighlight">\(\mu\)</span> and <span class="math notranslate nohighlight">\(\sigma_e^2\)</span> and set them to zero, we can get the following equations:</p></li>
</ol>
<div class="math notranslate nohighlight">
\[\hat{\mu} = \frac{\mathbf{1}^T\mathbf{K}^{-1}\mathbf{y}}{\mathbf{1}^T\mathbf{K}^{-1}\mathbf{1}}, \,\, \hat{\sigma_e}^2 =\frac{(\mathbf{y}-\mathbf{1}\mu)^T\mathbf{K}^{-1}(\mathbf{y}-\mathbf{1}\mu)}{n}\]</div>
<ol class="arabic simple" start="2">
<li><p>step 2: substitute the estimated hyper-parameters into the negative log-likelihood, we can get the following equation:</p></li>
</ol>
<div class="math notranslate nohighlight">
\[\ln(L) = -\frac{n}{2} \ln(\hat{\sigma_e}^2) - \frac{1}{2} \ln |\mathbf{K}|\]</div>
<p>which can be optimized by optimization algorithms.</p>
</section>
<section id="inference">
<h3>3. Inference<a class="headerlink" href="#inference" title="Link to this heading">¶</a></h3>
<p>The inference of the Gaussian process is defined as:</p>
<div class="math notranslate nohighlight">
\[ \begin{align}\begin{aligned}\begin{split}\begin{bmatrix}
           \mathbf{y} \\ \mathbf{y^{*}}
       \end{bmatrix} \sim N \left( \begin{bmatrix}
           m(X) \\ m(X^*)
       \end{bmatrix}, \begin{bmatrix}
           K(X,X) &amp; K(X, X^*) \\ K(X^*, X) &amp; K(X^*, X^*)
       \end{bmatrix} \right)\end{split}\\Therefore, the predicted mean and variance can be calculated as:\end{aligned}\end{align} \]</div>
<div class="math notranslate nohighlight">
\[\mathbf{y^*} = m(X^*) +  K(X^*, X)K(X,X)^{-1}(\mathbf{y} - m(X))\]</div>
<div class="math notranslate nohighlight">
\[\mathbf{s^2}^* = \sigma_e^2\left(1-K\left( X^*, X\right )K\left(X,X\right)^{-1}K\left(X, X^*\right)  \right)\]</div>
</section>
<section id="implementation">
<h3>4. Implementation<a class="headerlink" href="#implementation" title="Link to this heading">¶</a></h3>
<p>The Kriging model is implemented in <a class="reference internal" href="mfpml.models.html#mfpml.models.gaussian_process.GaussianProcessRegression" title="mfpml.models.gaussian_process.GaussianProcessRegression"><code class="xref py py-attr docutils literal notranslate"><span class="pre">GaussianProcessRegression</span></code></a> class, where we have to assign <em>noise_prior=0.0</em>. The following example is given to illustrate the usage of the Kriging model:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># Import required libraries</span>
<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="nn">plt</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">from</span> <span class="nn">mfpml.models.gaussian_process</span> <span class="kn">import</span> <span class="n">GaussianProcessRegression</span>
<span class="kn">from</span> <span class="nn">mfpml.problems.sf_functions</span> <span class="kn">import</span> <span class="n">Forrester</span>
<span class="kn">from</span> <span class="nn">mfpml.models.basis_functions</span> <span class="kn">import</span> <span class="n">Ordinary</span>

<span class="c1"># Define the target function</span>
<span class="n">func</span> <span class="o">=</span> <span class="n">Forrester</span><span class="p">()</span>

<span class="c1"># Sample points for training</span>
<span class="n">sample_x</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mf">0.0</span><span class="p">,</span> <span class="mf">0.4</span><span class="p">,</span> <span class="mf">0.6</span><span class="p">,</span> <span class="mf">1.0</span><span class="p">])</span><span class="o">.</span><span class="n">reshape</span><span class="p">((</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">))</span>
<span class="n">sample_y</span> <span class="o">=</span> <span class="n">func</span><span class="o">.</span><span class="n">f</span><span class="p">(</span><span class="n">sample_x</span><span class="p">)</span>

<span class="c1"># Generate test points</span>
<span class="n">test_x</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1001</span><span class="p">)</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
<span class="n">test_y</span> <span class="o">=</span> <span class="n">func</span><span class="o">.</span><span class="n">f</span><span class="p">(</span><span class="n">test_x</span><span class="p">)</span>

<span class="c1"># Initialize and train the Gaussian Process Regression model</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">GaussianProcessRegression</span><span class="p">(</span><span class="n">design_space</span><span class="o">=</span><span class="n">func</span><span class="o">.</span><span class="n">input_domain</span><span class="p">,</span> <span class="n">regr</span><span class="o">=</span><span class="n">Ordinary</span><span class="p">(),</span> <span class="n">noise_prior</span><span class="o">=</span><span class="mf">0.0</span><span class="p">)</span>
<span class="n">model</span><span class="o">.</span><span class="n">train</span><span class="p">(</span><span class="n">sample_x</span><span class="p">,</span> <span class="n">sample_y</span><span class="p">)</span>

<span class="c1"># Make predictions</span>
<span class="n">predictions</span><span class="p">,</span> <span class="n">std_dev</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">test_x</span><span class="p">,</span> <span class="n">return_std</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>

<span class="c1"># Plot the results</span>
<span class="n">fig</span><span class="p">,</span> <span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">5</span><span class="p">,</span> <span class="mi">4</span><span class="p">))</span>
<span class="n">ax</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">test_x</span><span class="p">,</span> <span class="n">test_y</span><span class="p">,</span> <span class="s2">&quot;k-&quot;</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="sa">r</span><span class="s2">&quot;$f(x)$&quot;</span><span class="p">)</span>                  <span class="c1"># True function</span>
<span class="n">ax</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">test_x</span><span class="p">,</span> <span class="n">predictions</span><span class="p">,</span> <span class="s2">&quot;b--&quot;</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="sa">r</span><span class="s2">&quot;$\hat</span><span class="si">{f}</span><span class="s2">(x)$&quot;</span><span class="p">)</span>      <span class="c1"># Predicted function</span>
<span class="n">ax</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">sample_x</span><span class="p">,</span> <span class="n">sample_y</span><span class="p">,</span> <span class="s2">&quot;ro&quot;</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s2">&quot;Samples&quot;</span><span class="p">)</span>              <span class="c1"># Training samples</span>
<span class="n">ax</span><span class="o">.</span><span class="n">fill_between</span><span class="p">(</span>
    <span class="n">test_x</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">),</span>
    <span class="p">(</span><span class="n">predictions</span> <span class="o">-</span> <span class="mf">1.96</span> <span class="o">*</span> <span class="n">std_dev</span><span class="p">)</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">),</span>
    <span class="p">(</span><span class="n">predictions</span> <span class="o">+</span> <span class="mf">1.96</span> <span class="o">*</span> <span class="n">std_dev</span><span class="p">)</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">),</span>
    <span class="n">color</span><span class="o">=</span><span class="s2">&quot;g&quot;</span><span class="p">,</span>
    <span class="n">alpha</span><span class="o">=</span><span class="mf">0.25</span><span class="p">,</span>
    <span class="n">label</span><span class="o">=</span><span class="s2">&quot;95</span><span class="si">% c</span><span class="s2">onfidence interval&quot;</span>
<span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">legend</span><span class="p">(</span><span class="n">loc</span><span class="o">=</span><span class="s2">&quot;best&quot;</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="s2">&quot;x&quot;</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="s2">&quot;y&quot;</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">grid</span><span class="p">()</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
<a class="reference internal image-reference" href="_images/kriging.png"><img alt="pridiction of Kriging model" class="align-center" src="_images/kriging.png" style="width: 400px;" />
</a>
</section>
</section>
<section id="gaussian-process-regression-for-noisy-data">
<h2>Gaussian process regression for noisy data<a class="headerlink" href="#gaussian-process-regression-for-noisy-data" title="Link to this heading">¶</a></h2>
<section id="basics-of-general-gaussian-process-regression">
<h3>1. basics of general Gaussian process regression<a class="headerlink" href="#basics-of-general-gaussian-process-regression" title="Link to this heading">¶</a></h3>
<p>If we want to model a problem with noise within the outputs, where the problem can be formulated as:</p>
<div class="math notranslate nohighlight">
\[y = f(x) + \epsilon\]</div>
<p>where <span class="math notranslate nohighlight">\(\epsilon\)</span> is the noise, which is assumed to be a Gaussian distribution with zero mean and variance <span class="math notranslate nohighlight">\(\sigma_a^2\)</span>.
Usually, this noise from data is called aleatory uncertainty, which is irreducible. So the general Gaussian process regression is proposed to model the aleatory uncertainty.
The essence of the general Gaussian process regression is to model the noise as a white noise process, which is defined as:</p>
<div class="math notranslate nohighlight">
\[k(\mathrm{x}, \mathrm{x}') = \sigma_a^2 \delta(\mathrm{x}, \mathrm{x}')\]</div>
<p>Intuitively, the white noise Correlation matrix is a diagonal matrix with
diagonal elements <span class="math notranslate nohighlight">\(\sigma_a^2\)</span></p>
<p>So the mix-kernel can be expressed as:</p>
<div class="math notranslate nohighlight">
\[K_{mix}\left (\mathbf{X}, \mathbf{X} \right) =K_{RBF}\left (\mathbf{X}, \mathbf{X} \right)  + K_{noise}\left (\mathbf{X}, \mathbf{X} \right)\]</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>The general Gaussian process regression is implemented in <code class="xref py py-attr docutils literal notranslate"><span class="pre">GaussianProcessRegressor</span></code> class.
The hyper-parameter estimation and inference process of general Gaussian process regression model is the same as the Kriging model, just the correlation matrix is different.
replace the original correlation matrix with the mix-kernel correlation matrix.</p>
</div>
</section>
<section id="id1">
<h3>1. Implementation<a class="headerlink" href="#id1" title="Link to this heading">¶</a></h3>
<p>The general Gaussian process regression model is implemented in <code class="xref py py-attr docutils literal notranslate"><span class="pre">GaussianProcessRegressor</span></code> class. The following example is given to illustrate the usage of the general Gaussian process regression model:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">mfpml.models.gaussian_process</span> <span class="kn">import</span> <span class="n">GaussianProcessRegression</span>
<span class="kn">from</span> <span class="nn">mfpml.optimization.evolutionary_algorithms</span> <span class="kn">import</span> <span class="n">DE</span>
<span class="kn">from</span> <span class="nn">mfpml.design_of_experiment.sf_samplers</span> <span class="kn">import</span> <span class="n">LatinHyperCube</span>

<span class="c1"># sampling</span>
<span class="n">sampler</span> <span class="o">=</span> <span class="n">LatinHyperCube</span><span class="p">(</span><span class="n">design_space</span><span class="o">=</span><span class="n">func</span><span class="o">.</span><span class="n">_input_domain</span><span class="p">)</span>
<span class="n">sample_x</span> <span class="o">=</span> <span class="n">sampler</span><span class="o">.</span><span class="n">get_samples</span><span class="p">(</span><span class="n">num_samples</span><span class="o">=</span><span class="mi">30</span><span class="p">)</span>
<span class="n">test_x</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">101</span><span class="p">,</span> <span class="n">endpoint</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>

<span class="c1"># get samples by adding noise to the true function</span>
<span class="n">sample_y</span> <span class="o">=</span> <span class="n">func</span><span class="o">.</span><span class="n">f</span><span class="p">(</span><span class="n">sample_x</span><span class="p">)</span> <span class="o">+</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">normal</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mf">0.2</span><span class="p">,</span>
                                              <span class="n">sample_x</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span><span class="o">.</span><span class="n">reshape</span><span class="p">((</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">))</span>
<span class="n">test_y</span> <span class="o">=</span> <span class="n">func</span><span class="o">.</span><span class="n">f</span><span class="p">(</span><span class="n">test_x</span><span class="p">)</span> <span class="o">+</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">normal</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mf">0.2</span><span class="p">,</span>
                                          <span class="n">test_x</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span><span class="o">.</span><span class="n">reshape</span><span class="p">((</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">))</span>
<span class="n">test_mean</span> <span class="o">=</span> <span class="n">func</span><span class="o">.</span><span class="n">f</span><span class="p">(</span><span class="n">test_x</span><span class="p">)</span>

<span class="c1"># get samples by adding noise to the true function</span>
<span class="n">sample_y</span> <span class="o">=</span> <span class="n">func</span><span class="o">.</span><span class="n">f</span><span class="p">(</span><span class="n">sample_x</span><span class="p">)</span> <span class="o">+</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">normal</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mf">0.2</span><span class="p">,</span>
                                              <span class="n">sample_x</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span><span class="o">.</span><span class="n">reshape</span><span class="p">((</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">))</span>
<span class="n">test_y</span> <span class="o">=</span> <span class="n">func</span><span class="o">.</span><span class="n">f</span><span class="p">(</span><span class="n">test_x</span><span class="p">)</span> <span class="o">+</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">normal</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mf">0.2</span><span class="p">,</span>
                                          <span class="n">test_x</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span><span class="o">.</span><span class="n">reshape</span><span class="p">((</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">))</span>
<span class="n">test_mean</span> <span class="o">=</span> <span class="n">func</span><span class="o">.</span><span class="n">f</span><span class="p">(</span><span class="n">test_x</span><span class="p">)</span>

<span class="c1"># initialize optimizer</span>
<span class="n">optimizer</span> <span class="o">=</span> <span class="n">DE</span><span class="p">(</span><span class="n">num_gen</span><span class="o">=</span><span class="mi">1000</span><span class="p">,</span> <span class="n">num_pop</span><span class="o">=</span><span class="mi">50</span><span class="p">,</span> <span class="n">crossover_rate</span><span class="o">=</span><span class="mf">0.5</span><span class="p">,</span>
              <span class="n">strategy</span><span class="o">=</span><span class="s2">&quot;DE/best/1/bin&quot;</span><span class="p">)</span>
<span class="c1"># initialize the regressor</span>
<span class="n">gp_model</span> <span class="o">=</span> <span class="n">GaussianProcessRegression</span><span class="p">(</span>
    <span class="n">design_space</span><span class="o">=</span><span class="n">func</span><span class="o">.</span><span class="n">_input_domain</span><span class="p">,</span> <span class="n">optimizer_restart</span><span class="o">=</span><span class="mi">20</span><span class="p">,</span> <span class="n">noise_prior</span><span class="o">=</span><span class="kc">None</span><span class="p">)</span>
<span class="c1"># train the model</span>
<span class="n">gp_model</span><span class="o">.</span><span class="n">train</span><span class="p">(</span><span class="n">sample_x</span><span class="p">,</span> <span class="n">sample_y</span><span class="p">)</span>
<span class="c1"># get the prediction</span>
<span class="n">sf_pre</span><span class="p">,</span> <span class="n">sf_std</span> <span class="o">=</span> <span class="n">gp_model</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">test_x</span><span class="p">,</span> <span class="n">return_std</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>

<span class="c1"># plot the results</span>
<span class="n">fig</span><span class="p">,</span> <span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">5</span><span class="p">,</span> <span class="mi">4</span><span class="p">))</span>
<span class="n">ax</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">test_x</span><span class="p">,</span> <span class="n">test_y</span><span class="p">,</span> <span class="s2">&quot;+&quot;</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s2">&quot;true noise data&quot;</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">test_x</span><span class="p">,</span> <span class="n">test_mean</span><span class="p">,</span> <span class="s2">&quot;k-&quot;</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="sa">r</span><span class="s2">&quot;$f(x)$&quot;</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">test_x</span><span class="p">,</span> <span class="n">sf_pre</span><span class="p">,</span> <span class="s2">&quot;b--&quot;</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="sa">r</span><span class="s2">&quot;$\hat</span><span class="si">{f}</span><span class="s2">(x)$&quot;</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">sample_x</span><span class="p">,</span> <span class="n">sample_y</span><span class="p">,</span> <span class="s2">&quot;.&quot;</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s2">&quot;samples&quot;</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">fill_between</span><span class="p">(</span>
    <span class="n">test_x</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">),</span>
    <span class="p">(</span><span class="n">sf_pre</span> <span class="o">-</span> <span class="mf">1.96</span> <span class="o">*</span> <span class="n">sf_std</span><span class="p">)</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">),</span>
    <span class="p">(</span><span class="n">sf_pre</span> <span class="o">+</span> <span class="mf">1.96</span> <span class="o">*</span> <span class="n">sf_std</span><span class="p">)</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">),</span>
    <span class="n">alpha</span><span class="o">=</span><span class="mf">0.25</span><span class="p">,</span>
    <span class="n">color</span><span class="o">=</span><span class="s2">&quot;g&quot;</span><span class="p">,</span>
    <span class="n">label</span><span class="o">=</span><span class="s2">&quot;95</span><span class="si">% c</span><span class="s2">onfidence interval&quot;</span><span class="p">,</span>
<span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">fill_between</span><span class="p">(</span>
    <span class="n">test_x</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">),</span>
    <span class="p">(</span><span class="n">sf_pre</span> <span class="o">-</span> <span class="mf">1.96</span> <span class="o">*</span> <span class="n">gp_model</span><span class="o">.</span><span class="n">noise</span><span class="p">)</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">),</span>
    <span class="p">(</span><span class="n">sf_pre</span> <span class="o">+</span> <span class="mf">1.96</span> <span class="o">*</span> <span class="n">gp_model</span><span class="o">.</span><span class="n">noise</span><span class="p">)</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">),</span>
    <span class="n">alpha</span><span class="o">=</span><span class="mf">0.3</span><span class="p">,</span>
    <span class="n">color</span><span class="o">=</span><span class="s2">&quot;m&quot;</span><span class="p">,</span>
    <span class="n">label</span><span class="o">=</span><span class="s2">&quot;95</span><span class="si">% a</span><span class="s2">leatotic interval&quot;</span><span class="p">,</span>
<span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">legend</span><span class="p">(</span><span class="n">loc</span><span class="o">=</span><span class="s2">&quot;best&quot;</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">grid</span><span class="p">(</span><span class="s1">&#39;--&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s2">&quot;x&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s2">&quot;y&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">savefig</span><span class="p">(</span><span class="s2">&quot;general_gaussian_process_regression.png&quot;</span><span class="p">,</span> <span class="n">dpi</span><span class="o">=</span><span class="mi">300</span><span class="p">,</span> <span class="n">bbox_inches</span><span class="o">=</span><span class="s2">&quot;tight&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
<a class="reference internal image-reference" href="_images/mix_kriging.png"><img alt="prediction of general Gaussian process regression model" class="align-center" src="_images/mix_kriging.png" style="width: 400px;" />
</a>
</section>
</section>
</section>


           </div>
          </div>
          <footer><div class="rst-footer-buttons" role="navigation" aria-label="Footer">
        <a href="problems.html" class="btn btn-neutral float-left" title="Problems class structure" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left" aria-hidden="true"></span> Previous</a>
        <a href="multi_fidelity_kriging_models.html" class="btn btn-neutral float-right" title="Multi fidelity surrogates" accesskey="n" rel="next">Next <span class="fa fa-arrow-circle-right" aria-hidden="true"></span></a>
    </div>

  <hr/>

  <div role="contentinfo">
    <p>&#169; Copyright 2024, Jiaxiang Yi.</p>
  </div>

  Built with <a href="https://www.sphinx-doc.org/">Sphinx</a> using a
    <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a>
    provided by <a href="https://readthedocs.org">Read the Docs</a>.
   

</footer>
        </div>
      </div>
    </section>
  </div>
  <script>
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script> 

</body>
</html>