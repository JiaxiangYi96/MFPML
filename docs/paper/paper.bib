
@article{saves2024smt,
        author = {P. Saves and R. Lafage and N. Bartoli and Y. Diouane and J. Bussemaker and T. Lefebvre and J. T. Hwang and J. Morlier and J. R. R. A. Martins},
        title = {{SMT 2.0: A} Surrogate Modeling Toolbox with a focus on Hierarchical and Mixed Variables Gaussian Processes},
        journal = {Advances in Engineering Sofware},
        year = {2024},
        volume = {188},
        pages = {103571},
        doi = {https://doi.org/10.1016/j.advengsoft.2023.103571}}

@Article{GiselleFernandezGodino2023,
  author   = {Giselle Fernández-Godino, M.},
  journal  = {Advances in Computational Science and Engineering},
  title    = {Review of multi-fidelity models},
  year     = {2023},
  number   = {4},
  pages    = {351-400},
  volume   = {1},
  doi      = {10.3934/acse.2023015},
  file     = {:Files/10.3934_acse.2023015.pdf:PDF},
  groups   = {multi-fidelity surrogate model},
  keywords = {Multi-fidelity, variable-complexity, variable-fidelity, surrogate models, optimization, uncertainty quantification, review, survey},
  url      = {https://www.aimsciences.org/article/doi/10.3934/acse.2023015},
}
@Book{Rasmussen2005,
  author    = {Rasmussen, Carl Edward and Williams, Christopher K. I.},
  publisher = {The MIT Press},
  title     = {{Gaussian Processes for Machine Learning}},
  year      = {2005},
  isbn      = {9780262256834},
  month     = {11},
  abstract  = {{A comprehensive and self-contained introduction to Gaussian processes, which provide a principled, practical, probabilistic approach to learning in kernel machines.Gaussian processes (GPs) provide a principled, practical, probabilistic approach to learning in kernel machines. GPs have received increased attention in the machine-learning community over the past decade, and this book provides a long-needed systematic and unified treatment of theoretical and practical aspects of GPs in machine learning. The treatment is comprehensive and self-contained, targeted at researchers and students in machine learning and applied statistics. The book deals with the supervised-learning problem for both regression and classification, and includes detailed algorithms. A wide variety of covariance (kernel) functions are presented and their properties discussed. Model selection is discussed both from a Bayesian and a classical perspective. Many connections to other well-known techniques from machine learning and statistics are discussed, including support-vector machines, neural networks, splines, regularization networks, relevance vector machines and others. Theoretical issues including learning curves and the PAC-Bayesian framework are treated, and several approximation methods for learning with large datasets are discussed. The book contains illustrative examples and exercises, and code and datasets are available on the Web. Appendixes provide mathematical background and a discussion of Gaussian Markov processes.}},
  doi       = {10.7551/mitpress/3206.001.0001},
  file      = {:Files/RW.pdf:PDF},
  groups    = {Gaussian process},
  url       = {https://doi.org/10.7551/mitpress/3206.001.0001},
}
@Article{Forrester2007,
  author   = {Forrester, Alexander I.J and Sóbester, András and Keane, Andy J},
  journal  = {Proceedings of the Royal Society A: Mathematical, Physical and Engineering Sciences},
  title    = {Multi-fidelity optimization via surrogate modelling},
  year     = {2007},
  number   = {2088},
  pages    = {3251-3269},
  volume   = {463},
  abstract = {This paper demonstrates the application of correlated Gaussian process based approximations to optimization where multiple levels of analysis are available, using an extension to the geostatistical method of co-kriging. An exchange algorithm is used to choose which points of the search space to sample within each level of analysis. The derivation of the co-kriging equations is presented in an intuitive manner, along with a new variance estimator to account for varying degrees of computational ‘noise’ in the multiple levels of analysis. A multi-fidelity wing optimization is used to demonstrate the methodology.},
  doi      = {10.1098/rspa.2007.1900},
  eprint   = {https://royalsocietypublishing.org/doi/pdf/10.1098/rspa.2007.1900},
  file     = {:Files/rspa.2007.1900.pdf:PDF},
  groups   = {Gaussian process},
  url      = {https://royalsocietypublishing.org/doi/abs/10.1098/rspa.2007.1900},
}


@Article{Han2012,
  author  = {Han, Zhong-Hua and G\"{o}rtz, Stefan},
  journal = {AIAA Journal},
  title   = {Hierarchical Kriging Model for Variable-Fidelity Surrogate Modeling},
  year    = {2012},
  number  = {9},
  pages   = {1885-1896},
  volume  = {50},
  doi     = {10.2514/1.J051354},
  eprint  = {https://doi.org/10.2514/1.J051354},
  file    = {:Files/han-görtz-2012-hierarchical-kriging-model-for-variable-fidelity-surrogate-modeling.pdf:PDF},
  groups  = {multi-fidelity surrogate model},
  url     = {https://doi.org/10.2514/1.J051354},
}
@Article{Jones1998,
  author   = {Jones, Donald R. and Schonlau, Matthias and Welch, William J.},
  journal  = {Journal of Global Optimization},
  title    = {Efficient Global Optimization of Expensive Black-Box Functions},
  year     = {1998},
  issn     = {1573-2916},
  number   = {4},
  pages    = {455--492},
  volume   = {13},
  abstract = {In many engineering optimization problems, the number of function evaluations is severely limited by time or cost. These problems pose a special challenge to the field of global optimization, since existing methods often require more function evaluations than can be comfortably afforded. One way to address this challenge is to fit response surfaces to data collected by evaluating the objective and constraint functions at a few points. These surfaces can then be used for visualization, tradeoff analysis, and optimization. In this paper, we introduce the reader to a response surface methodology that is especially good at modeling the nonlinear, multimodal functions that often occur in engineering. We then show how these approximating functions can be used to construct an efficient global optimization algorithm with a credible stopping rule. The key to using response surfaces for global optimization lies in balancing the need to exploit the approximating surface (by sampling where it is minimized) with the need to improve the approximation (by sampling where prediction error may be high). Striking this balance requires solving certain auxiliary problems which have previously been considered intractable, but we show how these computational obstacles can be overcome.},
  doi      = {10.1023/A:1008306431147},
  file     = {:Files/A_1008306431147.pdf:PDF},
  groups   = {Gaussian process},
  refid    = {Jones1998},
  url      = {https://doi.org/10.1023/A:1008306431147},
}
@article{harris2020array,
 title         = {Array programming with {NumPy}},
 author    = {Charles R. Harris and K. Jarrod Millman and St{\'{e}}fan J.
                 van der Walt and Ralf Gommers and Pauli Virtanen and David
                 Cournapeau and Eric Wieser and Julian Taylor and Sebastian
                 Berg and Nathaniel J. Smith and Robert Kern and Matti Picus
                 and Stephan Hoyer and Marten H. van Kerkwijk and Matthew
                 Brett and Allan Haldane and Jaime Fern{\'{a}}ndez del
                 R{\'{i}}o and Mark Wiebe and Pearu Peterson and Pierre
                 G{\'{e}}rard-Marchant and Kevin Sheppard and Tyler Reddy and
                 Warren Weckesser and Hameer Abbasi and Christoph Gohlke and
                 Travis E. Oliphant},
 year          = {2020},
 month         = sep,
 journal       = {Nature},
 volume        = {585},
 number        = {7825},
 pages         = {357--362},
 doi           = {10.1038/s41586-020-2649-2},
 publisher     = {Springer Science and Business Media {LLC}},
 url           = {https://doi.org/10.1038/s41586-020-2649-2}
}


@article{2020SciPy-NMeth,
  author  = {Virtanen, Pauli and Gommers, Ralf and Oliphant, Travis E. and
            Haberland, Matt and Reddy, Tyler and Cournapeau, David and
            Burovski, Evgeni and Peterson, Pearu and Weckesser, Warren and
            Bright, Jonathan and {van der Walt}, St{\'e}fan J. and
            Brett, Matthew and Wilson, Joshua and Millman, K. Jarrod and
            Mayorov, Nikolay and Nelson, Andrew R. J. and Jones, Eric and
            Kern, Robert and Larson, Eric and Carey, C J and
            Polat, {\.I}lhan and Feng, Yu and Moore, Eric W. and
            {VanderPlas}, Jake and Laxalde, Denis and Perktold, Josef and
            Cimrman, Robert and Henriksen, Ian and Quintero, E. A. and
            Harris, Charles R. and Archibald, Anne M. and
            Ribeiro, Ant{\^o}nio H. and Pedregosa, Fabian and
            {van Mulbregt}, Paul and {SciPy 1.0 Contributors}},
  title   = {{{SciPy} 1.0: Fundamental Algorithms for Scientific
            Computing in Python}},
  journal = {Nature Methods},
  year    = {2020},
  volume  = {17},
  pages   = {261--272},
  adsurl  = {https://rdcu.be/b08Wh},
  doi     = {10.1038/s41592-019-0686-2},
}

@article{scikit-learn,
 title={Scikit-learn: Machine Learning in {P}ython},
 author={Pedregosa, F. and Varoquaux, G. and Gramfort, A. and Michel, V.
         and Thirion, B. and Grisel, O. and Blondel, M. and Prettenhofer, P.
         and Weiss, R. and Dubourg, V. and Vanderplas, J. and Passos, A. and
         Cournapeau, D. and Brucher, M. and Perrot, M. and Duchesnay, E.},
 journal={Journal of Machine Learning Research},
 volume={12},
 pages={2825--2830},
 year={2011}
}