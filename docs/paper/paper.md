---
title: 'mfpml: Multi-fidelity probabilistic machine learning toolkit'
tags:
  - Python
  - multi-fidelity modeling
  - Gaussian process regression
  - Bayesian optimization
authors:
  - name: Jiaxiang Yi
    orcid: 0009-0007-9287-4328
    affiliation: "1"
    corresponding: true
  - name: Ji Cheng
    orcid: 0000-0002-2525-8070
    affiliation: "2"
affiliations:
 - name: Faculty of Mechanical Engineering, Delft University of Technology, the Netherlands
   index: 1
 - name: City University of Hong Kong, Hong Kong
   index: 2

date: 23 December 2024
bibliography: paper.bib

---

# Summary

The `mfpml` (multi-fidelity probabilistic machine learning) package provides a Python platform for implementing classic single- and multi-fidelity Bayesian machine learning surrogates and applying them to Bayesian optimization. Although numerous methods have been developed in the domain of multi-fidelity machine learning [@GiselleFernandezGodino2023], no open-source software offers a comprehensive suite of tools for this purpose. This package addresses this gap by providing a platform to replicate existing single- and multi-fidelity Bayesian methods based on Gaussian process regression. Furthermore, it serves as a handy tool for developing new methods in the field of multi-fidelity probabilistic machine learning.

![Logo of mfpml ([`mfpml`](https://pypi.org/project/mfpml/1.0.0/)). \label{fig:mfpml_logo}](logo.png)

# Statement of Need

`mfpml` is written in Python and depends on a few third-party packages, such as NumPy [@harris2020array] and SciPy [@2020SciPy-NMeth]. It includes detailed notebooks and autogenerated Sphinx documentation, allowing users to replicate existing methods and develop new ones with ease. Specifically, it provides essential modules for building machine learning models, including design of experiments, benchmark problems, models, and optimization.

Key features of `mfpml` include:

1. **Basic Methods**: Fundamental implementations of popular methods, such as Gaussian process regression [@Rasmussen2005], Co-Kriging [@Forrester2007], and corresponding extensions [@Han2012].
2. **Advanced Methods**: Advanced techniques for Bayesian optimization [@Jones1998], including single-fidelity and multi-fidelity optimization.
3. **Future Development**: Ongoing work includes adding constrained optimization and multi-objective optimization methods, which will be included in future versions.

# Acknowledgements

We thank the Python code of conduct provided by [BessaGroup](https://github.com/bessagroup/python_code_of_conduct) and acknowledge the support from our institutions.

# References

